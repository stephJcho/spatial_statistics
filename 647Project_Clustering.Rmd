---
title: "647Project_Clustering"
author: "Stephen Cho, Minjee Kim"
date: "2024-11-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Intro


```{r packages, include = FALSE}
library(wavelets)
library(reshape2)
library(tidyverse)
library(lubridate)
library(basemapR)
library(sf)
library(ggspatial)
library(prettymapr)
library(ggplot2)
library(igraph)
library(geosphere)
library(ggraph)
```

## Data Cleaning

This step was already performed during the EDA process; first, we added a new variable, 'Duration'.

```{r pressure, echo=FALSE}
data <- read.csv("cleaned_NYC_Yellow_Cab_2024-08.csv")
data <- data %>%
  mutate(
    trip_id = row_number(),
    tpep_pickup_datetime = as.POSIXct(tpep_pickup_datetime, format = "%Y-%m-%d %H:%M:%S"),
    tpep_dropoff_datetime = as.POSIXct(tpep_dropoff_datetime, format = "%Y-%m-%d %H:%M:%S"),
    Duration = as.numeric(difftime(tpep_dropoff_datetime, tpep_pickup_datetime, units = "mins"))
  ) %>%
  filter(!is.na(Duration), Duration > 0)
summary(data$Duration)
```

After detecting abnormal values, such as the max duration being 5734 'minutes', we treat them as outliers and filter them out.

```{r}
duration_limits <- quantile(data$Duration, probs = c(0.25, 0.75), na.rm = TRUE)
duration_iqr <- IQR(data$Duration, na.rm = TRUE)
distance_limits <- quantile(data$trip_distance, probs = c(0.25, 0.75), na.rm = TRUE)
distance_iqr <- IQR(data$trip_distance, na.rm = TRUE)

data <- data %>%
  filter(
    Duration >= (duration_limits[1] - 1.5 * duration_iqr) & Duration <= (duration_limits[2] + 1.5 * duration_iqr),
    trip_distance >= (distance_limits[1] - 1.5 * distance_iqr) & trip_distance <= (distance_limits[2] + 1.5 * distance_iqr)
  )

dim(data)
summary(data$Duration)
```
Around 140,000 data points were filtered out, resulting in a more reasonable summary for Duration.

```{r include=FALSE}
#Sys.getlocale("LC_TIME")
Sys.setlocale("LC_TIME", "C") #to get weekdays in English
#Sys.setlocale("LC_TIME", "Korean_Korea.utf8") #to reset
#weekdays(Sys.Date()+0:6) #for double check
```

# 2. Pickup demands

Now, we can perform spatio-temporal clustering of pickup demands. First, we work on the temporal part of the data by separating time variables into hour and weekday and creating demand matrix for pickup locations. After the processing, we can extract wavelet coefficients.

```{r}
pickup_data <- data %>% select(PU_Longitude, PU_Latitude, tpep_pickup_datetime, trip_id)

pickup_data <- pickup_data %>%
  mutate(
    pickup_hour = hour(tpep_pickup_datetime),
    pickup_day = wday(tpep_pickup_datetime, label = TRUE)
  )

hourly_demand <- pickup_data %>%
  group_by(PU_Longitude, PU_Latitude, pickup_hour, pickup_day) %>%
  summarize(demand = n(), .groups = "drop")

demand_matrix <- hourly_demand %>%
  unite("location", PU_Longitude, PU_Latitude, remove = FALSE) %>% # Combine coordinates for unique location ID
  pivot_wider(names_from = c(pickup_hour, pickup_day), values_from = demand, values_fill = 0)

### temporal 
temporal_data <- as.matrix(demand_matrix[, -c(1:3)])

wavelet_coeffs <- apply(temporal_data, 1, function(row) {
  dwt(as.numeric(row), filter = "haar")  # Haar wavelet as an example
})

wavelet_matrix <- do.call(rbind, lapply(wavelet_coeffs, function(x) x@W))  # Extract wavelet coefficients
```

Finally, we can create the graph and minimum spanning tree:

```{r}
library(igraph)
library(geosphere)

distances <- distm(cbind(demand_matrix$PU_Longitude, demand_matrix$PU_Latitude))
# edge list
edge_list <- which(lower.tri(distances), arr.ind = TRUE)  # Get row and column indices
edge_weights <- distances[lower.tri(distances)]  # Corresponding weights

# creating the graph 
spatial_graph <- graph_from_edgelist(edge_list, directed = FALSE)
E(spatial_graph)$weight <- edge_weights

# creating minimum spanning tree
mst_pickup <- mst(spatial_graph, weights = E(spatial_graph)$weight)
shortest_path_distances <- distances(mst_pickup, weights = E(mst_pickup)$weight)
dist_matrix <- as.dist(shortest_path_distances)

heatmap(as.matrix(dist_matrix), main = "Pickup Location Pairwise Distances in MST")

hc <- hclust(dist_matrix, method = "ward.D2")
num_clusters <- 5
spatial_clusters <- cutree(hc, k = num_clusters)

demand_matrix$spatial_cluster <- spatial_clusters
merged_data <- inner_join(pickup_data, demand_matrix, by = c("PU_Longitude", "PU_Latitude"))
```

We can get a heatmap of pairwise distances of pickup locations,

```{r}
ggplot(demand_matrix, aes(x = PU_Longitude, y = PU_Latitude, color = factor(spatial_cluster))) +
  geom_point(size = 1) +
  labs(
    title = "Spatial Clustering of Pickup Locations",
    x = "Longitude",
    y = "Latitude",
    color = "Cluster"
  ) +
  theme_minimal()
```

and eventually a visualization of spatial clusters of pickup locations in NYC.

```{r}
ggplot(merged_data, aes(x = pickup_hour, fill = factor(spatial_cluster))) +
  geom_bar(position = "dodge") +
  labs(
    title = "Pickup Demand by Hour and Cluster",
    x = "Hour of the Day",
    y = "Number of Pickups",
    fill = "Cluster"
  ) +
  theme_minimal()
```

Lastly, we combine the pickup hour as a factor to visualize the pickup demand by hour and cluster.


#3. Drop-off Demands

We can do the same for drop-off portion of the data:

```{r}
dropoff_data <- data %>% select(DO_Longitude, DO_Latitude, tpep_dropoff_datetime, trip_id)

dropoff_data <- dropoff_data %>%
  mutate(
    dropoff_hour = hour(tpep_dropoff_datetime),
    dropoff_day = wday(tpep_dropoff_datetime, label = TRUE)
  )

hourly_demand_dropoff <- dropoff_data %>%
  group_by(DO_Longitude, DO_Latitude, dropoff_hour, dropoff_day) %>%
  summarize(demand = n(), .groups = "drop")

demand_matrix_dropoff <- hourly_demand_dropoff %>%
  unite("location", DO_Longitude, DO_Latitude, remove = FALSE) %>%
  pivot_wider(names_from = c(dropoff_hour, dropoff_day), values_from = demand, values_fill = 0)

temporal_data_dropoff <- as.matrix(demand_matrix_dropoff[, -c(1:3)])

wavelet_coeffs_dropoff <- apply(temporal_data_dropoff, 1, function(row) {
  dwt(as.numeric(row), filter = "haar")
})

wavelet_matrix_dropoff <- do.call(rbind, lapply(wavelet_coeffs_dropoff, function(x) x@W))

distances_dropoff <- distm(cbind(demand_matrix_dropoff$DO_Longitude, demand_matrix_dropoff$DO_Latitude))
edge_list_dropoff <- which(lower.tri(distances_dropoff), arr.ind = TRUE)
edge_weights_dropoff <- distances_dropoff[lower.tri(distances_dropoff)]

spatial_graph_dropoff <- graph_from_edgelist(edge_list_dropoff, directed = FALSE)
E(spatial_graph_dropoff)$weight <- edge_weights_dropoff

mst_dropoff <- mst(spatial_graph_dropoff, weights = E(spatial_graph_dropoff)$weight)
shortest_path_distances_dropoff <- distances(mst_dropoff, weights = E(mst_dropoff)$weight)
dist_matrix_dropoff <- as.dist(shortest_path_distances_dropoff)

heatmap(as.matrix(dist_matrix_dropoff))

hc_dropoff <- hclust(dist_matrix_dropoff, method = "ward.D2")
num_clusters <- 5
spatial_clusters_dropoff <- cutree(hc_dropoff, k = num_clusters)

demand_matrix_dropoff$spatial_cluster <- spatial_clusters_dropoff
merged_data_dropoff <- inner_join(dropoff_data, demand_matrix_dropoff, by = c("DO_Longitude", "DO_Latitude"))
```

```{r}
ggplot(demand_matrix_dropoff, aes(x = DO_Longitude, y = DO_Latitude, color = factor(spatial_cluster))) +
  geom_point(size = 1) +
  labs(
    title = "Spatial Clustering of Drop-off Locations",
    x = "Longitude",
    y = "Latitude",
    color = "Cluster"
  ) +
  theme_minimal()
```

```{r}
ggplot(merged_data_dropoff, aes(x = dropoff_hour, fill = factor(spatial_cluster))) +
  geom_bar(position = "dodge") +
  labs(
    title = "Demand by Hour and Drop-off Cluster",
    x = "Hour of the Day",
    y = "Number of Drop-offs",
    fill = "Cluster"
  ) +
  theme_minimal()
```

#4. Matching the clusters

Lastly, we connect the clusters to further analyze the traffic flow.

```{r}
traffic_data <- data %>%
  left_join(merged_data, by = c("trip_id")) %>%
  left_join(merged_data_dropoff, by = c("trip_id"))

traffic_flow <- traffic_data %>%
  group_by(spatial_cluster.x, spatial_cluster.y) %>%
  summarize(trip_count = n(), .groups = "drop") %>%
  arrange(desc(trip_count))

library(ggraph)
traffic_flow$trip_count <- round(traffic_flow$trip_count)
flow_graph <- graph_from_data_frame(d = traffic_flow, directed = TRUE)
```

```{r}
ggraph(flow_graph, layout = "fr") +
  geom_edge_link(aes(width = trip_count, color = trip_count), alpha = 0.8) +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  scale_edge_color_gradient(
    low = "lightblue",
    high = "darkblue",
    trans = "log",
    labels = scales::comma  # Keep original counts formatted with commas
  ) +
  scale_edge_width(range = c(0.5, 2), guide = "none") +
  labs(
    title = "Directed Traffic Flow Between Clusters",
    edge_color = "Trip Count"
  ) +
  theme_minimal()
```